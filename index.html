<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN" "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="robots" content="index, follow" />
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="keywords" content="Zhikang Niu, ç‰›å¿—åº·, Audio Signal Processing, Speech Multimodal Large Language Model, Deep Learning, machine learning, Xidian University">
<link rel="stylesheet" href="./Files/jemdoc.css" type="text/css" />
<script src="jquery.min.js"></script>
<link rel="shortcut icon" href="./images/book.png">
<title>Zhikang Niu (ç‰›å¿—åº·)</title>
</head>
 
 
<body>

<a id="home" class="anchor"></a>
<div id="container"> 
<div class="container"> 

<table class="imgtable"><tr><td>
<a href="./"><img src="./images/fix.jpg" alt="" height="220px" /></a>&nbsp;</td>
<td align="left"><p><a href="./"><font size="4">Zhikang Niu (</font><font size="4"; font style="font-family:Microsoft YaHei">ç‰›å¿—åº·</font><font size="4">)</font></a><br />
<!-- <a href="https://www.cs.ox.ac.uk/people/yiyuan.yang/" target="_blank">Official Website</a> -->
<br />
Incoming Phd Student. <a href="https://www.cs.sjtu.edu.cn/" target="_blank">Cross Media (X-)Language Intelligence Lab</a><br />
<a href="https://www.cs.sjtu.edu.cn/" target="_blank">Department of Computer Science and Engineering</a>, <a href="https://www.sjtu.edu.cn/" target="_blank">Shanghai Jiao Tong University.</a>
<br /><br />
<class="staffshortcut">
 <A HREF="#Interest">Research Interest</A> | 
 <A HREF="#Education and intern">Education and Intern</A> | 
 <A HREF="#Publications">Publications</A> | 
 <A HREF="#Projects">Projects</A> | 
 <A HREF="#Honors and awards">Honors and Awards</A> |
 <A HREF="#Activities">Activities</A> | 
 <!-- <A HREF="#Invited talks and lives">Invited Talks and Lives</A> -->
<br />
<br />

If you have any things or questions to discuss, do not hesitate to contact me.<br />
Email: <a href="mailto:zkniu@stu.xidian.edu.cn">zkniu@stu.xidian.edu.cn</a> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <a href="mailto:nzk020109@gmail.com">nzk020109@gmail.com</a><br />
<br />
<!-- [<a href="mailto:zkniu@stu.xidian.edu.cn" target="_blank">Email</a>] -->
<!-- [<a href="https://scholar.google.com/citations?user=FUuGvZIAAAAJ&hl" target="_blank">Google Scholar</a>]  -->
<!-- [<a href="https://www.researchgate.net/profile/Yiyuan-Yang-3" target="_blank">ResearchGate</a>]  -->
[<a href="https://github.com/ZhikangNiu" target="_blank">GitHub</a>] 
[<a href="./assets/WeChat_QR.jpg" target="_blank">Wechat</a>]
[<a href="./assets/resume-zh_CN.pdf" target="_blank">Chinese Resume</a>]
</td></tr></table>


 
<A NAME="Interest"><h2>Research Interest</h2></A>
I work in the field of Audio Singal Processing, Audio Codec Model, Self-supervised learning, Large Language Model, Multimodal Model, Machine learning, and Deep learning supervised by <a href="https://chenxie95.github.io/" target="_blank">Prof. Xie Chen</a>, I will try my best in the next five exciting years! ðŸ’ª. Currently, I focus on the following research topics:
<ul>
<li>Audio Codec Model which converts continuation latent representation to discrete token</li>
<li>Audio Self-supervised learning</li>
<li>Speech Multimodal Large Language Model</li>
</ul>


 
<A NAME="Education and intern"><h2>Education and Intern</h2></A>
<ul>
<!-- <li>2023.01-Now &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; D.Phil in the <a href="https://www.cs.ox.ac.uk/" target="_blank">Department of Computer Science</a> at the <a href="https://www.ox.ac.uk/" target="_blank">University of Oxford</a> (Also as a member in the <a href="https://www.merton.ox.ac.uk/" target="_blank">Merton College</a>), supervised by <a href="https://www.cs.ox.ac.uk/people/niki.trigoni/" target="_blank"> Profs. Niki Trigoni</a> and <a href="https://www.cs.ox.ac.uk/people/andrew.markham/" target="_blank">Andrew Markham</a>, with the fully funded <a href="https://www.ox.ac.uk/clarendon" target="_blank">Clarendon Scholarship</a> and Merton College Scholarship.</li> -->
<!-- <li>2022.10-2023.02 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; AI Research Intern in the <a href="https://damo.alibaba.com/labs/decision-intelligence" target="_blank">Decision Intelligence Lab</a>-<a href="https://damo.alibaba.com/?lang=en" target="_blank">Alibaba DAMO Academy</a> supervised by <a href="https://sites.google.com/site/qingsongwen8/" target="_blank">Dr. Qingsong Wen</a> and <a href="https://scholar.google.com/citations?user=8JbrsgUAAAAJ&hl=en&oi=ao" target="_blank">Dr. Liang Sun</a>. Focus on Time-series for Anomaly Detection.</li>    -->
<!-- <li>2019.09-2022.07 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; M.E. in the <a href="http://www.au.tsinghua.edu.cn/" target="_blank">Department of Automation</a> at the <a href="https://www.tsinghua.edu.cn/en/" target="_blank">Tsinghua University</a>. GPA 3.98/4.0, Rank 1/65.</li> -->
<li>2023.08-Now &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; AI Research Intern in the Natural Language Computing Group - <a href="https://www.msra.cn/" target="_blank">Microsoft Research Asia</a> supervised by <a href="https://www.microsoft.com/en-us/research/people/shujliu/" target="_blank">Shujie Liu</a> and <a href="https://www.microsoft.com/en-us/research/people/lozhou/" target="_blank">Long Zhou</a>. Focus on Audio Codec and Speech Multimodal Large Language Model.</li> 
<!-- <li>2019.01-2019.02 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Exchange student in the Artificial intelligence field</a> at the <a href="https://www.cam.ac.uk/" target="_blank">University of Cambridge</a>.</li> -->
<li>2020.09-Now &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Undergraduate in the <a href="https://sai.xidian.edu.cn/" target="_blank">School of Artificial Intelligence</a> at the <a href="https://www.xidian.edu.cn/" target="_blank">Xidian University</a>. (Artificial Intelligence) GPA 3.8/4.0.</li>
</ul>

 


<A NAME="Publications"><h2>Publications</h2></A>
<b>Models and Methods for Speech SSL</b>:
<!-- <b>Models and Methods for Speech SSL</b> -->
<font size="3"> 
<ul>
<!-- <p style="text-indent: -2.5rem;margin-left: 0rem;"> -->
<li> Guanrou Yang, Ziyang Ma, Zhisheng Zheng, Yakun Song, <b>Zhikang Niu</b>, Xie Chen*.<br>
<b>Fast-HuBERT: An Efficient Training Framework for Self-Supervised Speech Representation Learning.</b><br>
<i>IEEE Automatic Speech Recognition and Understanding Workshop (<b>ASRU Workshop</b>)</i>, 
2023.
[<a href= "https://arxiv.org/abs/2309.13860" target="_blank">Link</a>]
[<a href= "https://browse.arxiv.org/pdf/2309.13860.pdf" target="_blank">PDF</a>]
[<a href="https://scholar.googleusercontent.com/scholar.bib?q=info:5igU4Z9Xe2sJ:scholar.google.com/&output=citation&scisdr=ClEoLDBxEOzd6Ydut8w:AFWwaeYAAAAAZSFor8xL1SdSCeYwkCTVnwkGQSo&scisig=AFWwaeYAAAAAZSFor1SCJ3LMXRhW0icv0dF0A4c&scisf=4&ct=citation&cd=-1&hl=zh-CN">BibTeX</a>]
</p>
</ul>
</font>

 <!-- <b>Text to Speech</b> -->
<font size="3"> 
<ul>
<!-- <p style="text-indent: -2.5rem;margin-left: 0rem;"> -->
<li> Chenpeng Du, Yiwei Guo, Hankun Wang, Yifan Yang, <b>Zhikang Niu</b>, Shuai Wang, Hui Zhang, Xie Chen.<br>
<b>VALL-T: Decoder-Only Generative Transducer for Robust and Decoding-Controllable Text-to-Speech.</b><br>
<!-- <i>IEEE Automatic Speech Recognition and Understanding Workshop (<b>ASRU Workshop</b>)</i>,  -->
<!-- Submitted to ICML 2024. -->
[<a href= "https://arxiv.org/abs/2401.14321" target="_blank">Link</a>]
[<a href= "https://arxiv.org/pdf/2401.14321.pdf" target="_blank">PDF</a>]
[<a href="https://scholar.googleusercontent.com/scholar.bib?q=info:423LVFzDAh0J:scholar.google.com/&output=citation&scisdr=ClEXtQ9QEMjm1kg_lBQ:AFWwaeYAAAAAZeE5jBRZItSkmZL-6cy9PIFICTI&scisig=AFWwaeYAAAAAZeE5jKmKyS0hsW4Yk5zbPIk00-M&scisf=4&ct=citation&cd=-1&hl=en">BibTeX</a>]
</p>
</ul>
</font>

<A NAME="Projects"><h2>Projects</h2></A>
<b>Open-Source Projects</b>:
<font size="3"> 
<ul>

<li><a href= "https://github.com/datawhalechina/thorough-pytorch" target="_blank"><b>thorough-pytorch</b>:</a> A Chinese PyTorch tutorial and it has already collected <b><font color="#FF0000">2,000 more stars and 333 forks</font></b> on GitHub.
</li>

<li>More open-source contents can be found on my <a href= "https://github.com/NoFish-528" target="_blank">GitHub</a>.
</li>
</ul>
</font>
 
 
<b>Research Projects</b>
<font size="3"> 
<ul>

<li><a href= "https://github.com/NoFish-528/encodec-pytorch" target="_blank"><b>encodec-pytorch</b>:</a> An unofficial PyTorch implementation of the <a href= "https://arxiv.org/pdf/2210.13438.pdf" target="_blank">High Fidelity Neural Audio Compression</a> and it has already collected <b><font color="#FF0000">95 stars</font></b> on GitHub. [<a href= "https://huggingface.co/zkniu/encodec-pytorch/tree/main" target="_blank">checkpoint</a>] 
</li>

</ul>
</font>
<A NAME="Honors and awards"><h2>Honors and Awards</h2></A>
<font size="3"> 
<ul>
<li>2022, National Scholarship, Ministry of Education in China.</li>
<li>2021, Meritorious Winner, Interdisciplinary Contest In Modeling.</li>
<li>2021, 2023, The First Prize Scholarship, Xidian University.</li>
</ul>
</font>

<A NAME="Activities"><h2>Activities</h2></A>
<font size="3"> 
<ul>
<li>2021.11-Now, <a href= "https://datawhale.club/" target="_blank">Datawhale member</a> (an open-source AI organization), helped data science fans get involved in the AI community.</li>
</ul>
</font>

<!-- <A NAME="Invited talks and lives"><h2>Invited Talks and Lives</h2></A>
<font size="3"> 
<ul>
Will be updated in the future. -->
<!-- <li>2023.10, A Speech Multimodal paper sharing presentation.
    [<a href= "https://www.bilibili.com/video/BV18M4y1s7UN/?spm_id_from=333.337.search-card.all.click&vd_source=642fa389e9e78cff4881c038963ac312" target="_blank">Video</a>]
    [<a href= "https://mp.weixin.qq.com/s/N3OkPlCQwN_c5sxxOZFy2A" target="_blank">Link</a>]
    [<a href= "./Files/Yang-KDDAITIME-2023.jpg" target="_blank">Picture</a>]
</li> 
</li>  
<li>2022.09, New Students Experience Sharing in the SIGS, Tsinghua University.
</li>    
<li>2022.09, Invited Talk in the <a href= "https://www.worldaic.com.cn/" target="_blank">2022 World Artificial Intelligence Conference</a>. (Dishui Lake AI Developer Innovation Forum)
    [<a href= "https://online2022.worldaic.com.cn/forumdetail?uuid=c412de9a58604519940f06cac98fd1fb&type=video" target="_blank">Video1</a>]
    [<a href= "https://www.bilibili.com/video/BV1xW4y1B72o?spm_id_from=333.1007.top_right_bar_window_custom_collection.content.click&vd_source=642fa389e9e78cff4881c038963ac312" target="_blank">Video2</a>]
    [<a href= "https://mp.weixin.qq.com/s/xQwPAt6750uShbCiaAXV0w" target="_blank">Link1</a>]
    [<a href= "https://mp.weixin.qq.com/s/d-xrmlGXz28SJsM-VaJLeg" target="_blank">Link2</a>]
    [<a href= "https://mp.weixin.qq.com/s/2I_tp4K3rkEeAPTT1dXgww" target="_blank">Link3</a>]
    [<a href= "https://mp.weixin.qq.com/s/1br3Xs5-t2UjgH-wqEF04g" target="_blank">Link4</a>]
</li> -->

</ul>
</font>

</br></br></br></br>
<!-- <script type="text/javascript" src="//rf.revolvermaps.com/0/0/6.js?i=5yaha4wk64q&amp;m=7&amp;c=e63100&amp;cr1=ffffff&amp;f=arial&amp;l=0&amp;bv=90&amp;lx=-420&amp;ly=420&amp;hi=20&amp;he=7&amp;hc=a8ddff&amp;rs=80" async="async"></script> -->
<script type='text/javascript' id='clustrmaps' src='//cdn.clustrmaps.com/map_v2.js?cl=000000&w=500&t=tt&d=mIxY2j5UCMhyHcb61rSeFnsPsRzjjBuB4sPdfTs1dMk&co=ffffff&ct=000000&cmo=3acc3a&cmn=ff5353'></script>
<div id="article"></div>
<div id="back_top">
<div class="arrow"></div>
<div class="stick"></div>
</div>

<script>
$(function(){
    $(window).scroll(function(){  //If scroll
        var scrollt = document.documentElement.scrollTop + document.body.scrollTop; //Getting Height after scroll
        if( scrollt >400 )
        {  
            $("#back_top").fadeIn(400); 
        }
        else
        {
            $("#back_top").stop().fadeOut(400);
        }
    });

    $("#back_top").click(function(){ 

        $("html,body").animate({scrollTop:"0px"}, 200);

    }); 

});
</script>


<!--
All Rights Reserved by Yiyuan Yang. Part of page is generated by <a href="http://jemdoc.jaboc.net/">jemdoc</a>.
-->

<!--
<font size="2"; color="#A0A0A0";>
<p style="text-align:center">Updating time: 2024.03.01</p>
</font>
-->

</body>
</html>
